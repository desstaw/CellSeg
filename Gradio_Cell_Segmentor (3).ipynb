{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIeIT2iZk8_o",
        "outputId": "5b2dcc77-66d9-4e9d-c702-a2b8fbea80eb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "q7ALf4zpkpVX",
        "outputId": "ea23402b-e384-4dbf-e423-a04c700b9cb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.4.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.4-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting gradio-client==1.4.2 (from gradio)\n",
            "  Downloading gradio_client-1.4.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.2)\n",
            "Collecting huggingface-hub>=0.25.1 (from gradio)\n",
            "  Downloading huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Collecting markupsafe~=2.0 (from gradio)\n",
            "  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.10)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.1)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (10.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.9.2)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart==0.0.12 (from gradio)\n",
            "  Downloading python_multipart-0.0.12-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.2.2 (from gradio)\n",
            "  Downloading ruff-0.7.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<1.0,>=0.1.1 (from gradio)\n",
            "  Downloading safehttpx-0.1.1-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.41.2-py3-none-any.whl.metadata (6.0 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.5)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.32.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.4.2->gradio) (2024.10.0)\n",
            "Collecting websockets<13.0,>=10.0 (from gradio-client==1.4.2->gradio)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (3.16.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (4.66.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.23.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (2.2.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.4.0-py3-none-any.whl (56.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.7/56.7 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.4.2-py3-none-any.whl (319 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.8/319.8 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.12-py3-none-any.whl (23 kB)\n",
            "Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.4-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.7/94.7 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.26.2-py3-none-any.whl (447 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.5/447.5 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Downloading ruff-0.7.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.0/11.0 MB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.1-py3-none-any.whl (8.4 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.41.2-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.3/73.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.32.0-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydub, websockets, uvicorn, tomlkit, semantic-version, ruff, python-multipart, markupsafe, ffmpy, aiofiles, starlette, huggingface-hub, safehttpx, gradio-client, fastapi, gradio\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.24.7\n",
            "    Uninstalling huggingface-hub-0.24.7:\n",
            "      Successfully uninstalled huggingface-hub-0.24.7\n",
            "Successfully installed aiofiles-23.2.1 fastapi-0.115.4 ffmpy-0.4.0 gradio-5.4.0 gradio-client-1.4.2 huggingface-hub-0.26.2 markupsafe-2.1.5 pydub-0.25.1 python-multipart-0.0.12 ruff-0.7.2 safehttpx-0.1.1 semantic-version-2.10.0 starlette-0.41.2 tomlkit-0.12.0 uvicorn-0.32.0 websockets-12.0\n"
          ]
        }
      ],
      "source": [
        "!pip install gradio\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from skimage.io import imread\n",
        "from skimage.transform import resize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import shutil\n",
        "import gradio as gr\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.io import imread\n",
        "from skimage.measure import label, regionprops\n",
        "from skimage.segmentation import watershed\n",
        "from scipy.ndimage import distance_transform_edt\n",
        "import cv2\n",
        "import gradio as gr\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.io import imread, imsave\n",
        "from skimage.measure import label, regionprops\n",
        "from skimage.segmentation import watershed\n",
        "from scipy.ndimage import distance_transform_edt\n",
        "import cv2\n"
      ],
      "metadata": {
        "id": "BrWzJ-xblEYY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Home Page"
      ],
      "metadata": {
        "id": "W6eFABUQ2WeK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def home_page():\n",
        "    return gr.Markdown(\n",
        "        \"\"\"\n",
        "        # Cell Segmentor Dashboard\n",
        "        Welcome to the **Cell Segmentor Dashboard**. This dashboard leverages the power of a U-Net++ model for cell nuclei segmentation, refined through a custom-designed pipeline for image analysis. Built specifically for microscopy imaging in cell research, this tool aids in precise identification and separation of individual nuclei, even in densely packed cellular structures.\n",
        "\n",
        "        ### Key Features\n",
        "        - **Automated Nuclei Segmentation**: Upload a microscopy image, and the dashboard will automatically apply the U-Net++ model to detect and segment cell nuclei with high precision.\n",
        "        - **Refined Separation of Overlapping Nuclei**: Using watershed and distance transform methods, overlapping nuclei are carefully separated for individual analysis.\n",
        "        - **Interactive Nuclei Properties**: After segmentation, each nucleus is assigned a unique label, and users can explore detailed properties, including area, perimeter, compactness, eccentricity, solidity, and orientation.\n",
        "        - **Manual Drawing and Property Calculation**: For customized analysis, use the sketchpad tool to draw shapes on a transparent layer over an uploaded image and calculate shape properties.\n",
        "\n",
        "        ### Data and Training\n",
        "        - The segmentation model was trained on preprocessed microscopy images and labels from the **Data Science Bowl 2018** dataset, which provides robust, realistic examples of cellular structures.\n",
        "        - The dataset was split into training, validation, and test sets to ensure model generalization and accuracy across unseen data.\n",
        "\n",
        "        ### How to Use the Dashboard\n",
        "        - **Application**: Navigate to the **Segment Cells** tab to start with automated segmentation or to manually draw and analyze shapes.\n",
        "        - **About**: Visit this section to learn more about the underlying techniques and intended use cases.\n",
        "\n",
        "        ### Navigation\n",
        "        - **Segment Cells**: Begin your segmentation task by uploading an image for automated analysis.\n",
        "        - **Draw and Calculate Shape Properties**: Customize the analysis by drawing regions on an image for specific property calculations.\n",
        "\n",
        "        ### Acknowledgments\n",
        "        This tool was developed using U-Net++ architecture and leverages advanced image processing techniques to support scientific research in cellular biology. It is especially suited for applications in cell structure analysis, allowing for accurate morphological insights.\n",
        "\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "# Create the home page layout\n",
        "home_interface = gr.Interface(\n",
        "    fn=home_page,\n",
        "    inputs=[],\n",
        "    outputs=\"markdown\",\n",
        "    title=\"Cell Segmentor Dashboard - Home\"\n",
        ")\n",
        "\n",
        "home_interface.launch(share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "id": "i0yjETOeuLoY",
        "outputId": "a0f0cfe4-5682-4821-9f5c-ae5d283948de"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://91c9c7e1ba897fbaa1.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://91c9c7e1ba897fbaa1.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Application Page"
      ],
      "metadata": {
        "id": "t2-TI9QD2Uw6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.io import imread, imsave\n",
        "from skimage.measure import label, regionprops\n",
        "from skimage.segmentation import watershed\n",
        "from skimage.morphology import h_maxima\n",
        "from scipy.ndimage import distance_transform_edt, binary_fill_holes\n",
        "import cv2\n",
        "import io\n",
        "from PIL import Image\n",
        "\n",
        "# Define U-Net++ model architecture\n",
        "class UNetPlusPlus(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(UNetPlusPlus, self).__init__()\n",
        "\n",
        "        # Encoder path\n",
        "        self.encoder1 = self.double_conv(in_channels, 64)\n",
        "        self.encoder2 = self.double_conv(64, 128)\n",
        "        self.encoder3 = self.double_conv(128, 256)\n",
        "        self.encoder4 = self.double_conv(256, 512)\n",
        "\n",
        "        # Bottleneck\n",
        "        self.bottleneck = self.double_conv(512, 1024)\n",
        "\n",
        "        # Decoder path with nested convolutions (UNet++)\n",
        "        self.upconv4 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
        "        self.decoder4_1 = self.double_conv(1024, 512)\n",
        "        self.decoder4_2 = self.double_conv(1024, 512)\n",
        "\n",
        "        self.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
        "        self.decoder3_1 = self.double_conv(512, 256)\n",
        "        self.decoder3_2 = self.double_conv(512, 256)\n",
        "        self.decoder3_3 = self.double_conv(512, 256)\n",
        "\n",
        "        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
        "        self.decoder2_1 = self.double_conv(256, 128)\n",
        "        self.decoder2_2 = self.double_conv(256, 128)\n",
        "        self.decoder2_3 = self.double_conv(256, 128)\n",
        "        self.decoder2_4 = self.double_conv(256, 128)\n",
        "\n",
        "        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
        "        self.decoder1_1 = self.double_conv(128, 64)\n",
        "        self.decoder1_2 = self.double_conv(128, 64)\n",
        "        self.decoder1_3 = self.double_conv(128, 64)\n",
        "        self.decoder1_4 = self.double_conv(128, 64)\n",
        "        self.decoder1_5 = self.double_conv(128, 64)\n",
        "\n",
        "        # Output layer\n",
        "        self.out_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
        "\n",
        "    def double_conv(self, in_channels, out_channels, dropout_prob=0.1):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.ELU(inplace=True),\n",
        "            nn.Dropout(dropout_prob),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.ELU(inplace=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        e1 = self.encoder1(x)\n",
        "        e2 = self.encoder2(F.max_pool2d(e1, 2))\n",
        "        e3 = self.encoder3(F.max_pool2d(e2, 2))\n",
        "        e4 = self.encoder4(F.max_pool2d(e3, 2))\n",
        "\n",
        "        b = self.bottleneck(F.max_pool2d(e4, 2))\n",
        "\n",
        "        d4_1 = self.upconv4(b)\n",
        "        d4_1 = torch.cat([d4_1, e4], dim=1)\n",
        "        d4_1 = self.decoder4_1(d4_1)\n",
        "\n",
        "        d3_1 = self.upconv3(d4_1)\n",
        "        d3_1 = torch.cat([d3_1, e3], dim=1)\n",
        "        d3_1 = self.decoder3_1(d3_1)\n",
        "\n",
        "        d2_1 = self.upconv2(d3_1)\n",
        "        d2_1 = torch.cat([d2_1, e2], dim=1)\n",
        "        d2_1 = self.decoder2_1(d2_1)\n",
        "\n",
        "        d1_1 = self.upconv1(d2_1)\n",
        "        d1_1 = torch.cat([d1_1, e1], dim=1)\n",
        "        d1_1 = self.decoder1_1(d1_1)\n",
        "\n",
        "        out = torch.sigmoid(self.out_conv(d1_1))\n",
        "        out = F.interpolate(out, size=(x.size(2), x.size(3)), mode='bilinear', align_corners=False)\n",
        "        return out\n",
        "\n",
        "# Load the pre-trained model weights\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = UNetPlusPlus(in_channels=3, out_channels=1).to(device)\n",
        "model_path = '/content/drive/MyDrive/CellSeg/unet++_224_model_500epochs.pth'\n",
        "model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "model.eval()\n",
        "\n",
        "# Function for refined cell segmentation\n",
        "def segment_cells(image):\n",
        "    image_tensor = torch.from_numpy(image).permute(2, 0, 1).unsqueeze(0).float().to(device)\n",
        "    with torch.no_grad():\n",
        "        predicted_mask = torch.sigmoid(model(image_tensor)).squeeze().cpu().numpy() > 0.5\n",
        "\n",
        "    # Fill holes in the binary mask\n",
        "    filled_mask = binary_fill_holes(predicted_mask).astype(np.uint8)\n",
        "\n",
        "    # Compute the distance transform\n",
        "    distance_map = distance_transform_edt(filled_mask)\n",
        "\n",
        "    # Detect peaks using H-maxima transform\n",
        "    h_maxima_markers = h_maxima(distance_map, h=0.1 * distance_map.max())\n",
        "    markers = label(h_maxima_markers)\n",
        "\n",
        "    # Apply watershed segmentation\n",
        "    initial_labels = watershed(-distance_map, markers, mask=filled_mask)\n",
        "\n",
        "    # Refine segmentation for overlapping nuclei\n",
        "    final_labels = np.zeros_like(initial_labels)\n",
        "    current_label = 1\n",
        "\n",
        "    for region in regionprops(initial_labels):\n",
        "        nucleus_mask = (initial_labels == region.label)\n",
        "        distance_local = distance_transform_edt(nucleus_mask)\n",
        "        local_h_maxima = h_maxima(distance_local, h=0.3 * distance_local.max())\n",
        "        local_markers = label(local_h_maxima)\n",
        "        refined_labels = watershed(-distance_local, local_markers, mask=nucleus_mask)\n",
        "        refined_labels[refined_labels > 0] += current_label - 1\n",
        "        final_labels += refined_labels\n",
        "        current_label = final_labels.max() + 1\n",
        "\n",
        "    # Visualization of refined segmentation (color mask only)\n",
        "    fig_colored, ax = plt.subplots(figsize=(6, 6))\n",
        "    ax.imshow(final_labels, cmap='nipy_spectral')\n",
        "    ax.axis('off')\n",
        "    plt.close(fig_colored)\n",
        "\n",
        "    # Visualization of numbered binary mask\n",
        "    fig_numbered, ax = plt.subplots(figsize=(10, 10))\n",
        "    ax.imshow(filled_mask, cmap='gray')\n",
        "    for region in regionprops(final_labels):\n",
        "        y, x = region.centroid\n",
        "        ax.text(x, y, f'{region.label}', color='red', fontsize=12, ha='center')\n",
        "    ax.axis('off')\n",
        "    plt.close(fig_numbered)\n",
        "\n",
        "    # Calculating properties of refined labeled regions\n",
        "    properties = []\n",
        "    for region in regionprops(final_labels):\n",
        "        # Compute additional properties for each nucleus\n",
        "        area = region.area\n",
        "        perimeter = region.perimeter\n",
        "        compactness = (4 * np.pi * area) / (perimeter ** 2) if perimeter > 0 else 0\n",
        "        eccentricity = region.eccentricity\n",
        "        solidity = region.solidity\n",
        "        major_axis_length = region.major_axis_length\n",
        "        minor_axis_length = region.minor_axis_length\n",
        "        aspect_ratio = major_axis_length / minor_axis_length if minor_axis_length > 0 else 0\n",
        "        orientation = region.orientation\n",
        "\n",
        "        # Add computed properties to the properties list\n",
        "        properties.append({\n",
        "            \"area\": area,\n",
        "            \"perimeter\": perimeter,\n",
        "            \"compactness\": compactness,\n",
        "            \"eccentricity\": eccentricity,\n",
        "            \"solidity\": solidity,\n",
        "            \"major_axis_length\": major_axis_length,\n",
        "            \"minor_axis_length\": minor_axis_length,\n",
        "            \"aspect_ratio\": aspect_ratio,\n",
        "            \"orientation\": orientation,\n",
        "        })\n",
        "\n",
        "    # Generate individual masks for gallery\n",
        "    individual_masks = [(final_labels == i).astype(np.uint8) * 255 for i in range(1, final_labels.max() + 1)]\n",
        "\n",
        "    return fig_colored, fig_numbered, individual_masks, properties\n",
        "\n",
        "# Function to fetch properties of a selected nucleus\n",
        "def get_nucleus_properties(index, properties):\n",
        "    if 0 <= index < len(properties):\n",
        "        selected_props = properties[index]\n",
        "        return (\n",
        "            f\"Area: {selected_props['area']} pixels\\n\"\n",
        "            f\"Perimeter: {selected_props['perimeter']:.2f} pixels\\n\"\n",
        "            f\"Compactness: {selected_props['compactness']:.2f}\\n\"\n",
        "            f\"Eccentricity: {selected_props['eccentricity']:.2f}\\n\"\n",
        "            f\"Solidity: {selected_props['solidity']:.2f}\\n\"\n",
        "            f\"Major Axis Length: {selected_props['major_axis_length']:.2f} pixels\\n\"\n",
        "            f\"Minor Axis Length: {selected_props['minor_axis_length']:.2f} pixels\\n\"\n",
        "            f\"Aspect Ratio: {selected_props['aspect_ratio']:.2f}\\n\"\n",
        "            f\"Orientation: {selected_props['orientation']:.2f} radians\"\n",
        "        )\n",
        "    else:\n",
        "        return \"Select a nucleus to view its properties.\"\n",
        "\n",
        "# New function for drawing on an image and calculating shape properties\n",
        "def create_filled_binary_mask_and_calculate_properties(drawn_shape):\n",
        "    debug_output = io.StringIO()\n",
        "    try:\n",
        "        if isinstance(drawn_shape, dict) and 'layers' in drawn_shape:\n",
        "            drawn_shape = drawn_shape['layers']\n",
        "\n",
        "        if drawn_shape is None:\n",
        "            raise ValueError(\"No drawing layer found in the dictionary returned by Sketchpad\")\n",
        "\n",
        "        if not isinstance(drawn_shape, np.ndarray):\n",
        "            drawn_shape = np.array(drawn_shape, dtype=np.uint8)\n",
        "\n",
        "        if drawn_shape.ndim == 4 and drawn_shape.shape[0] == 1:\n",
        "            drawn_shape = np.squeeze(drawn_shape, axis=0)\n",
        "\n",
        "        if drawn_shape.ndim == 3 and drawn_shape.shape[2] == 4:\n",
        "            alpha_channel = drawn_shape[:, :, 3]\n",
        "            mask = alpha_channel > 0\n",
        "            gray_image = np.where(mask, 255, 0).astype(np.uint8)\n",
        "        elif drawn_shape.ndim == 3:\n",
        "            gray_image = cv2.cvtColor(drawn_shape, cv2.COLOR_RGB2GRAY)\n",
        "        else:\n",
        "            gray_image = drawn_shape\n",
        "\n",
        "        _, binary_mask = cv2.threshold(gray_image, 1, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "        contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "        filled_mask = np.zeros_like(binary_mask)\n",
        "        cv2.drawContours(filled_mask, contours, -1, color=255, thickness=cv2.FILLED)\n",
        "\n",
        "        if not contours:\n",
        "            return filled_mask, \"No contours found.\"\n",
        "\n",
        "        contour = max(contours, key=cv2.contourArea)\n",
        "\n",
        "        area = cv2.contourArea(contour)\n",
        "        perimeter = cv2.arcLength(contour, True)\n",
        "        compactness = (perimeter**2) / (4 * np.pi * area) if area > 0 else 0\n",
        "\n",
        "        hull = cv2.convexHull(contour)\n",
        "        hull_area = cv2.contourArea(hull)\n",
        "        solidity = area / hull_area if hull_area > 0 else 0\n",
        "\n",
        "        if len(contour) >= 5:\n",
        "            ellipse = cv2.fitEllipse(contour)\n",
        "            (center, axes, angle) = ellipse\n",
        "            major_axis_length = max(axes)\n",
        "            minor_axis_length = min(axes)\n",
        "            aspect_ratio = major_axis_length / minor_axis_length if minor_axis_length > 0 else 0\n",
        "            orientation = np.deg2rad(angle)\n",
        "            eccentricity = np.sqrt(1 - (minor_axis_length / major_axis_length)**2)\n",
        "        else:\n",
        "            major_axis_length = minor_axis_length = aspect_ratio = orientation = eccentricity = 0\n",
        "\n",
        "        properties_text = (\n",
        "            f\"Area: {area} pixels\\n\"\n",
        "            f\"Perimeter: {perimeter:.2f} pixels\\n\"\n",
        "            f\"Compactness: {compactness:.2f}\\n\"\n",
        "            f\"Eccentricity: {eccentricity:.2f}\\n\"\n",
        "            f\"Solidity: {solidity:.2f}\\n\"\n",
        "            f\"Major Axis Length: {major_axis_length:.2f}\\n\"\n",
        "            f\"Minor Axis Length: {minor_axis_length:.2f}\\n\"\n",
        "            f\"Aspect Ratio: {aspect_ratio:.2f}\\n\"\n",
        "            f\"Orientation: {orientation:.2f} radians\"\n",
        "        )\n",
        "\n",
        "        return filled_mask, properties_text\n",
        "    except Exception as e:\n",
        "        return np.zeros((512, 512), dtype=np.uint8), f\"Error: {e}\"\n",
        "\n",
        "'''\n",
        "# Gradio Interface for Segmenting Cells\n",
        "application_interface = gr.Interface(\n",
        "    fn=segment_cells,\n",
        "    inputs=gr.Image(label=\"Upload an Image\", type=\"numpy\"),\n",
        "    outputs=[\n",
        "        gr.Plot(label=\"Refined Segmentation Mask\"),\n",
        "        gr.Plot(label=\"Numbered Nuclei Mask\"),\n",
        "        gr.Gallery(label=\"Individual Nuclei Masks\", interactive=True),\n",
        "        gr.Textbox(label=\"Nucleus Properties\", placeholder=\"Select a nucleus to view its properties\")\n",
        "    ],\n",
        "    title=\"Cell Segmentor Dashboard - Application\",\n",
        "    description=\"Upload an image of cells. The model will segment the nuclei with refined watershed-based separation for overlapping objects, and label each nucleus with a unique number.\",\n",
        ")\n",
        "'''\n",
        "with gr.Blocks() as application_interface:\n",
        "    gr.Markdown(\"# Cell Segmentation with U-Net++ and Refined Post-Processing\")\n",
        "\n",
        "    with gr.Row():\n",
        "        input_image = gr.Image(label=\"Upload an Image\", type=\"numpy\")\n",
        "\n",
        "    with gr.Row():\n",
        "        segmented_colored = gr.Plot(label=\"Refined Segmentation Mask\")\n",
        "        segmented_numbered = gr.Plot(label=\"Numbered Binary Mask\")\n",
        "\n",
        "    individual_nuclei = gr.Gallery(label=\"Individual Nuclei Masks\", interactive=True)\n",
        "    nucleus_properties = gr.Textbox(label=\"Nucleus Properties\", value=\"Select a nucleus to view its properties.\")\n",
        "\n",
        "    # Store properties as a state variable\n",
        "    properties_state = gr.State()\n",
        "\n",
        "    # Load image and perform segmentation\n",
        "    def on_segment(image):\n",
        "        fig_colored, fig_numbered, individual_masks, properties = segment_cells(image)\n",
        "        return fig_colored, fig_numbered, individual_masks, properties\n",
        "\n",
        "    # Fetch properties when an image is selected\n",
        "    def on_select_image(evt: gr.SelectData, properties):\n",
        "        selected_index = evt.index\n",
        "        return get_nucleus_properties(selected_index, properties)\n",
        "\n",
        "    # Connect functions to components\n",
        "    input_image.upload(on_segment, inputs=input_image, outputs=[segmented_colored, segmented_numbered, individual_nuclei, properties_state])\n",
        "    individual_nuclei.select(on_select_image, inputs=properties_state, outputs=nucleus_properties)\n",
        "\n",
        "# Gradio Interface for Drawing on Image and Calculating Shape Properties\n",
        "with gr.Blocks() as manual_separation_interface:\n",
        "    gr.Markdown(\"## Upload an Image as Background, Draw on Transparent Layer, and Calculate Shape Properties\")\n",
        "\n",
        "    background_image_input = gr.Image(type=\"pil\", label=\"Upload Background Image\")\n",
        "    drawn_shape_input = gr.Sketchpad(label=\"Draw on Layer\", type=\"pil\")\n",
        "\n",
        "    filled_shape_output = gr.Image(label=\"Filled Shape\")\n",
        "    debug_output = gr.Textbox(label=\"Shape Properties and Debugging Output\")\n",
        "\n",
        "    generate_button = gr.Button(\"Generate segmented nucleus and relevant dimensions\")\n",
        "    generate_button.click(fn=create_filled_binary_mask_and_calculate_properties, inputs=[drawn_shape_input], outputs=[filled_shape_output, debug_output])\n",
        "\n",
        "    background_image_input.change(fn=lambda img: img.convert(\"RGBA\"), inputs=[background_image_input], outputs=[drawn_shape_input])\n",
        "\n",
        "# Combine both interfaces into a tabbed interface\n",
        "app_interface = gr.TabbedInterface([application_interface, manual_separation_interface], [\"Automatic Segmentation\", \"Manual Segmentation\"])\n",
        "app_interface.launch(share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "ghparfNxRKiY",
        "outputId": "0ae09d96-abf5-430d-b5e5-fd717119ab03"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-83-a73c8a689ac4>:97: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://e4a67503df61a473d2.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://e4a67503df61a473d2.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------------------------------------------\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "hgLPGyaxxqwk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "About Page"
      ],
      "metadata": {
        "id": "4ky68-7N2b2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def about_page():\n",
        "    return gr.Markdown(\n",
        "        \"\"\"\n",
        "        # About the Cell Segmentor Dashboard\n",
        "        The **Cell Segmentor Dashboard** is a powerful tool developed for the segmentation of cell nuclei from microscopy images. It leverages a U-Net++ architecture for precise detection and separation of individual nuclei, making it especially useful for researchers in cellular biology.\n",
        "\n",
        "        ### Model and Dataset\n",
        "        - **Model Architecture**: The dashboard uses a U-Net++ model, a deep convolutional neural network designed for image segmentation tasks. U-Net++ improves on traditional U-Net by adding nested convolutions and skip connections, enabling better segmentation accuracy, particularly for complex and overlapping cell structures.\n",
        "        - **Training Dataset**: The model was trained on the **Data Science Bowl 2018** dataset, which contains a diverse collection of cell images. The dataset was preprocessed to enhance contrast and resize images, making them suitable for the segmentation tasks in this application.\n",
        "\n",
        "        ### Training and Performance\n",
        "        - **Training Process**: The model was trained using a combination of Binary Cross-Entropy and Dice loss, optimized with Focal Loss to handle class imbalances. Training used a cyclic learning rate schedule to achieve high accuracy.\n",
        "        - **Evaluation Metrics**: Model performance is measured using the Dice coefficient and Intersection over Union (IoU) metrics. After training, the model achieved:\n",
        "          - **Mean Dice Score**: 0.8666\n",
        "          - **Mean IoU Score**: 0.7727\n",
        "        - **Early Stopping**: To prevent overfitting, early stopping was applied based on the Dice score.\n",
        "\n",
        "        ### Key Functionalities\n",
        "        - **Automated Segmentation**: Upload a microscopy image, and the model will automatically segment the cell nuclei. It employs watershed and distance transform techniques to refine segmentation and separate overlapping nuclei.\n",
        "        - **Visualization**: View the segmentation mask with individual nuclei labeled, allowing for easy identification and selection.\n",
        "        - **Property Analysis**: For each segmented nucleus, detailed properties are calculated, including area, perimeter, eccentricity, and solidity. This information helps in quantitative morphological analysis of the nuclei.\n",
        "        - **Manual Drawing Tool**: Users can also draw custom shapes on an image for a more focused analysis of specific areas. Shape properties are calculated for each custom region.\n",
        "\n",
        "        ### How to Use\n",
        "        - **Upload an Image**: Navigate to the **Segment Cells** tab to upload a microscopy image. The tool will then process the image and display the refined segmentation results.\n",
        "        - **Interactive Nucleus Selection**: Each nucleus is labeled, and users can select individual nuclei to view their properties.\n",
        "        - **Manual Drawing**: Use the **Draw and Calculate Shape Properties** tab to upload an image and manually draw shapes for custom analysis.\n",
        "\n",
        "        ### Evaluation Example\n",
        "        Below is an example of the training and validation performance plot showing both the loss and Dice coefficient over epochs. This demonstrates how the model's accuracy improved over time.\n",
        "\n",
        "        ![Training and Validation Loss and Dice Coefficient](https://raw.githubusercontent.com/desstaw/CellSeg/refs/heads/main/train_val.png)\n",
        "\n",
        "        ### Acknowledgments\n",
        "        This project was developed using open-source libraries like PyTorch, Torchvision, and Albumentations, alongside data provided by the Data Science Bowl. Special thanks to the open-source community and contributors to these libraries, making tools like this possible for advancing cell biology research.\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "# Create the about page layout\n",
        "about_interface = gr.Interface(\n",
        "    fn=about_page,\n",
        "    inputs=[],\n",
        "    outputs=\"markdown\",\n",
        "    title=\"Cell Segmentor Dashboard - About\"\n",
        ")\n",
        "\n",
        "about_interface.launch(share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "id": "f0ZtuRmTpLrY",
        "outputId": "d80dbef9-0074-494f-a8c0-6b0309c1656a"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://3e726a853f8abbb396.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://3e726a853f8abbb396.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combine pages into a tabbed interface"
      ],
      "metadata": {
        "id": "0SMjS5xDxgoJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the combined Gradio tabbed interface\n",
        "dashboard = gr.TabbedInterface(\n",
        "    [home_interface, app_interface, about_interface],\n",
        "    [\"Home\", \"Application\", \"About\"]\n",
        ")\n",
        "\n",
        "# Launch the complete dashboard\n",
        "dashboard.launch(share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "id": "gbyQGAwhpNHB",
        "outputId": "db231c49-ba02-485a-da46-6038ffafebf5"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://0e059be4ad88f16a96.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://0e059be4ad88f16a96.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    }
  ]
}